# Server 2 - 9g_8b_thinking_llama model
#
# Note: registry_url/router_url/host are overridden at runtime by docker entrypoint env:
#   - BABYSITTER_HOST (for registration)
#   - REGISTRY_URL / ROUTER_URL (remote URLs)

name = "server2-9g_8b_thinking"
host = "0.0.0.0"
port = 8100

registry_url = "http://localhost:18000"
router_url = "http://localhost:8000"

[babysitter]
max_restarts = 10000
restart_delay = 5
heartbeat_interval = 30

# Backend: InfiniLM Python inference server launched via the README command shape.
# InfiniLM repo is at /workspace/InfiniLM (default) or /mnt/InfiniLM (if mounted).
[backend]
type = "command"
# Use conda's Python to ensure InfiniCore/InfiniLM are available
command = "/opt/conda/bin/python"
args = [
  "/workspace/InfiniLM/python/infinilm/server/inference_server.py",
  "--metax",
  "--model_path", "/models/9g_8b_thinking",
  "--max_batch_size", "8",
  "--tp", "1",
  "--temperature", "1.0",
  "--top_p", "0.8",
  "--top_k", "1",
  "--host", "0.0.0.0",
  "--port", "8100",
]
work_dir = "/workspace/InfiniLM"
# Set environment variables: ensure conda's Python path and InfiniCore/InfiniLM paths are available
# These match what env-set.sh sets up (sourced before Python imports)
[backend.env]
PATH = "/opt/conda/bin:/root/.infini/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
PYTHONPATH = "/workspace/InfiniLM/python:/workspace/InfiniCore/python"
INFINI_ROOT = "/root/.infini"
LD_LIBRARY_PATH = "/opt/conda/lib:/root/.infini/lib:/opt/hpcc/lib:/opt/hpcc/htgpu_llvm/lib"
MACA_HOME = "/opt/hpcc"
MACA_PATH = "/opt/hpcc"
HPCC_PATH = "/opt/hpcc"
C_INCLUDE_PATH = "/opt/hpcc/include/hcr:/opt/hpcc/tools/cu-bridge/include"
XMAKE_ROOT = "y"
