# Slave (Server 2) - Qwen3-32B model
# Settings copied from master-Qwen3-32B.toml
#
# Note: registry_url/router_url/host are overridden at runtime by docker entrypoint env:
#   - BABYSITTER_HOST (for registration)
#   - REGISTRY_URL / ROUTER_URL (remote URLs)

name = "slave-Qwen3-32B"
host = "0.0.0.0"
port = 8200

registry_url = "http://localhost:18000"
router_url = "http://localhost:8000"

[babysitter]
max_restarts = 10000
restart_delay = 5
heartbeat_interval = 30

# Backend: InfiniLM Python inference server
[backend]
type = "command"
command = "/opt/conda/bin/python"
args = [
  "/workspace/InfiniLM/python/infinilm/server/inference_server.py",
  "--metax",
  "--model_path", "/models/Qwen3-32B",
  "--max_batch_size", "32",
  "--tp", "4",
  "--max_tokens", "2048",
  "--temperature", "1.0",
  "--top_p", "0.8",
  "--top_k", "1",
  "--host", "0.0.0.0",
  "--port", "8200",
]
work_dir = "/workspace/InfiniLM"

[backend.env]
PATH = "/opt/conda/bin:/root/.infini/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
PYTHONPATH = "/workspace/InfiniLM/python:/workspace/InfiniCore/python"
INFINI_ROOT = "/root/.infini"
LD_LIBRARY_PATH = "/opt/conda/lib:/root/.infini/lib:/opt/hpcc/lib:/opt/hpcc/htgpu_llvm/lib:/opt/hpcc/ompi/lib"
MACA_HOME = "/opt/hpcc"
MACA_PATH = "/opt/hpcc"
HPCC_PATH = "/opt/hpcc"
C_INCLUDE_PATH = "/opt/hpcc/include/hcr:/opt/hpcc/tools/cu-bridge/include"
XMAKE_ROOT = "y"
# Note: GPU selection is controlled by the 'gpus' field in master-service_qwen.toml, not by HPCC_VISIBLE_DEVICES
HPCC_VISIBLE_DEVICES = "4,5,6,7"
