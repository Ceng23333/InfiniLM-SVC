# Dockerfile for InfiniLM-SVC deployment based on GPU factory image
# Builds InfiniLM-SVC on top of Metax GPU factory base image
#
# Base image: GPU factory image with HPCC, PyTorch, Python 3.10, Kylin Linux (ARM64)
# Example: cr.metax-tech.com/public-ai-release-wb/x201/vllm:hpcc2.32.0.11-torch2.4-py310-kylin2309a-arm64
#
# This Dockerfile uses install.sh script to build and install InfiniLM-SVC
# (simplified single-stage build using the installation script)
# Uses BuildKit cache mounts for faster rebuilds

# Base image: GPU factory image with HPCC, PyTorch, Python, etc.
# Default can be overridden via --build-arg BASE_IMAGE
ARG BASE_IMAGE=cr.metax-tech.com/public-ai-release-wb/x201/vllm:hpcc2.32.0.11-torch2.4-py310-kylin2309a-arm64
# Proxy settings (optional, can be set via --build-arg)
ARG HTTP_PROXY=""
ARG HTTPS_PROXY=""
ARG http_proxy=""
ARG https_proxy=""
ARG ALL_PROXY=""
ARG all_proxy=""
ARG NO_PROXY=""
ARG no_proxy=""

FROM ${BASE_IMAGE}

WORKDIR /app

# Set proxy environment variables if provided
ENV HTTP_PROXY=${HTTP_PROXY}
ENV HTTPS_PROXY=${HTTPS_PROXY}
ENV http_proxy=${http_proxy}
ENV https_proxy=${https_proxy}
ENV ALL_PROXY=${ALL_PROXY}
ENV all_proxy=${all_proxy}
ENV NO_PROXY=${NO_PROXY}
ENV no_proxy=${no_proxy}

# Copy all project files (needed for install.sh to work)
# install.sh will handle OS detection and install all required dependencies
COPY . .

# Set deployment case environment variable for install.sh
ENV DEPLOYMENT_CASE=infinilm-metax-deployment

# Use the base conda environment from the base image directly
# Ensure conda bin is first in PATH so git and other tools from conda are used
ENV PATH="/opt/conda/bin:/usr/bin:/usr/local/bin:${PATH}"

# Check if git is available in base conda environment (use conda bin directly)
RUN if [ -x /opt/conda/bin/git ]; then \
        echo "✓ git found in base conda environment at /opt/conda/bin/git"; \
    elif command -v git >/dev/null 2>&1; then \
        echo "✓ git found at $(command -v git)"; \
    else \
        echo "⚠ git not found in base conda environment, install.sh will attempt installation"; \
    fi

# Run install script to build and install binaries
# Note: install.sh will handle git installation if needed (with conda fallback)
# This will:
# - Install Rust if not present
# - Build release binaries
# - Install binaries to /usr/local/bin
# - Install Python dependencies (if INSTALL_PYTHON_DEPS=true)
# - Clone and install InfiniCore and InfiniLM if not present
# Note: For cache mounts, use Docker BuildKit with --build-arg CACHE_DIR=/path/to/cache
#       or mount host directories: -v /host/cache/cargo:/root/.cargo -v /host/cache/pip:/root/.cache/pip
RUN mkdir -p /workspace && \
    ./scripts/install.sh \
    --install-path /usr/local/bin \
        --deployment-case infinilm-metax-deployment \
        --install-infinicore true \
        --install-infinilm true \
        --infinicore-src /workspace/InfiniCore \
        --infinilm-src /workspace/InfiniLM

# Verify installation
RUN which infini-registry && \
    which infini-router && \
    which infini-babysitter && \
    infini-registry --help > /dev/null 2>&1 || true

# Copy deployment case specific files to /app
COPY deployment/cases/infinilm-metax-deployment/embeddings_server.py ./embeddings_server.py
# Copy env-set.sh and requirements-embeddings.txt if they exist (handle optional files with RUN)
RUN if [ -f deployment/cases/infinilm-metax-deployment/env-set.sh ]; then \
        cp deployment/cases/infinilm-metax-deployment/env-set.sh ./env-set.sh; \
    fi || true
RUN if [ -f deployment/cases/infinilm-metax-deployment/requirements-embeddings.txt ]; then \
        cp deployment/cases/infinilm-metax-deployment/requirements-embeddings.txt ./requirements-embeddings.txt; \
    fi || true

# Copy docker entrypoint script
COPY docker/docker_entrypoint_rust.sh ./docker_entrypoint.sh

# Make scripts executable
RUN chmod +x ./script/*.sh ./docker_entrypoint.sh 2>/dev/null || true

# Create necessary directories
RUN mkdir -p logs config /workspace/models

# Unset proxy environment variables at the end of build to avoid proxy interference at runtime
# Proxy is only needed during build for downloading dependencies
# At runtime, services should connect directly without proxy routing issues
# Set NO_PROXY to include localhost/127.0.0.1 to ensure local connections bypass proxy
ENV HTTP_PROXY=""
ENV HTTPS_PROXY=""
ENV http_proxy=""
ENV https_proxy=""
ENV ALL_PROXY=""
ENV all_proxy=""
ENV NO_PROXY="localhost,127.0.0.1,0.0.0.0"
ENV no_proxy="localhost,127.0.0.1,0.0.0.0"

# Set working directory
WORKDIR /app

# Set entrypoint
ENTRYPOINT ["/bin/bash", "/app/docker_entrypoint.sh"]

# Expose default ports
# REGISTRY_PORT (default: 18000)
# ROUTER_PORT (default: 8000)
# EMBEDDING_PORT (default: 20002)
EXPOSE 18000 8000 20002

# Labels
LABEL maintainer="InfiniLM Team"
LABEL description="InfiniLM-SVC: Built on GPU factory base image for infinilm-metax-deployment"
LABEL deployment-case="infinilm-metax-deployment"
LABEL base-image="${BASE_IMAGE}"

# Usage:
#   Build with default base image:
#     docker build -f deployment/cases/infinilm-metax-deployment/Dockerfile.gpu-factory \
#                  -t infinilm-svc:infinilm-demo .
#
#   Build with custom base image:
#     docker build -f deployment/cases/infinilm-metax-deployment/Dockerfile.gpu-factory \
#                  --build-arg BASE_IMAGE=your-registry/image:tag \
#                  -t infinilm-svc:infinilm-demo .
#
#   Run:
#     docker run -d --name infinilm-svc-master \
#                -e LAUNCH_COMPONENTS=all \
#                -e REGISTRY_PORT=18000 \
#                -e ROUTER_PORT=8000 \
#                infinilm-svc:infinilm-demo
